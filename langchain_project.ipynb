{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPGFehr75jJGG4sZiar3XtL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ishaqgopang/langchain/blob/main/langchain_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "5yYs-37iKw0S"
      },
      "outputs": [],
      "source": [
        "!pip install -Uq langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uq google-generativeai"
      ],
      "metadata": {
        "id": "PRb6SszoK8vH"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uq langchain_community"
      ],
      "metadata": {
        "id": "sdltbak1PtKQ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade --quiet  langchain-google-genai pillow"
      ],
      "metadata": {
        "id": "anGpJHsTQ_l3"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "rmmhRWbRUJEA",
        "outputId": "874f041d-6bab-47e1-9e98-c176914b84a4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'AIzaSyApfhs1hcgm77u6U8W_7WTjRcLyY6VmpiI'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "import os\n",
        "os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "67Rf5jFqT69_"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
        "result = llm.invoke(\"Write a ballad about LangChain\")\n",
        "print(result.content)"
      ],
      "metadata": {
        "id": "DE9_dy1OVXcQ",
        "outputId": "c2ea111b-be43-4975-9874-82c014b9b393",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The coder toiled, both late and long,\n",
            "With data vast, a tangled throng.\n",
            "To wrangle knowledge, make it sing,\n",
            "A mighty task, a daunting thing.\n",
            "\n",
            "He'd built his models, sleek and bright,\n",
            "But lacked the bridge to shining light.\n",
            "The answers hid, a buried hoard,\n",
            "His weary soul, it felt ignored.\n",
            "\n",
            "Then whispers came, upon the breeze,\n",
            "Of LangChain, meant to bring him ease.\n",
            "A framework new, a shining star,\n",
            "To link the parts, both near and far.\n",
            "\n",
            "With LLM's power, strong and deep,\n",
            "And chains of thought, its secrets keep.\n",
            "From prompts it built, a careful weave,\n",
            "And made the data start to breathe.\n",
            "\n",
            "He used its agents, swift and keen,\n",
            "To search and find, what lay unseen.\n",
            "With memory banks, both short and long,\n",
            "His programs hummed a joyful song.\n",
            "\n",
            "The chains he forged, both strong and true,\n",
            "Connected prompts to actions new.\n",
            "From simple tasks to complex schemes,\n",
            "LangChain fulfilled his coding dreams.\n",
            "\n",
            "No longer lost, in data's maze,\n",
            "He found his way, through coding's haze.\n",
            "With LangChain's help, his work took flight,\n",
            "And bathed his world in digital light.\n",
            "\n",
            "So raise a glass, to this new art,\n",
            "That binds the mind, and plays its part.\n",
            "LangChain's magic, clear and bold,\n",
            "A story in this ballad told.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "apikey= userdata.get('GOOGLE_API_KEY')\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    api_key= apikey,\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    temperature=0.9\n",
        ")\n",
        "\n",
        "result = llm.invoke(\"What is ML\")\n",
        "print(result.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xB2GzjbvMEtG",
        "outputId": "d371ae77-bed3-45fa-ddc5-7d2bff71ddef"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ML stands for **Machine Learning**.  It's a branch of artificial intelligence (AI) and computer science which focuses on the use of data and algorithms to imitate the way that humans learn, gradually improving its accuracy.  Instead of being explicitly programmed, ML systems are trained on data, allowing them to identify patterns, make predictions, and improve their performance over time without being explicitly programmed for each specific task.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"You are a helpful assistant. Answer the following question:\\n\\n{question}\"\n",
        ")\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=prompt_template)"
      ],
      "metadata": {
        "id": "I4OwlHFVdn_a",
        "outputId": "afa23aaa-f300-46fc-bce1-b05d04d17c82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-30-d807c0097176>:6: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  chain = LLMChain(llm=llm, prompt=prompt_template)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is LangChain?\"\n",
        "response = chain.run({\"question\": question})\n",
        "\n",
        "print(\"Answer:\", response)"
      ],
      "metadata": {
        "id": "6nuy01mEeMma",
        "outputId": "0f7b071a-e63e-4a0d-c5d8-df21b7baa099",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-31-2c7e5464e91b>:2: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  response = chain.run({\"question\": question})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: LangChain is a framework for developing applications powered by language models.  It's designed to make it easier to build applications that combine the capabilities of large language models (LLMs) with other sources of data and computation.  Essentially, it provides a structured way to connect LLMs to your data and the real world.\n",
            "\n",
            "Key features include:\n",
            "\n",
            "* **Modular Components:** LangChain offers reusable components like prompts, indexes, chains, agents, and memory. This allows developers to build complex applications by combining these pre-built blocks rather than starting from scratch.\n",
            "\n",
            "* **Data Connection:** LangChain facilitates connecting LLMs to various data sources, such as databases, APIs, and documents.  This allows the LLM to access and process information beyond its pre-training data.\n",
            "\n",
            "* **Chains and Agents:**  These features enable sequential operations and decision-making, allowing for more sophisticated applications. Chains orchestrate multiple calls to LLMs and other components, while agents act based on the LLM's output and interact with the environment to achieve a specific goal.\n",
            "\n",
            "* **Memory:** LangChain provides mechanisms for LLMs to remember previous interactions within a conversation or task, enabling context-aware responses.\n",
            "\n",
            "\n",
            "In short, LangChain simplifies the development of LLM-powered applications by providing a structured framework and reusable components, allowing developers to focus on the application logic rather than the intricacies of LLM interaction and data management.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}